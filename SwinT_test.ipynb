{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "patch embed\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 96, 56, 56])\n",
      "torch.Size([1, 3136, 96])\n",
      "H,W: 56 56\n",
      "==================================================\n",
      "basic layer [stage2 swinT block and stage3 patch merging]\n",
      "H,W: 28 28\n",
      "torch.Size([2, 784, 192])\n",
      "==============================\n",
      "mask\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4.]])\n",
      "torch.Size([1, 28, 28, 1])\n",
      "torch.Size([1, 4, 7, 4, 7, 1])\n",
      "torch.Size([16, 7, 7, 1])\n",
      "tensor([[4., 4., 4., 4., 5., 5., 5.],\n",
      "        [4., 4., 4., 4., 5., 5., 5.],\n",
      "        [4., 4., 4., 4., 5., 5., 5.],\n",
      "        [4., 4., 4., 4., 5., 5., 5.],\n",
      "        [7., 7., 7., 7., 8., 8., 8.],\n",
      "        [7., 7., 7., 7., 8., 8., 8.],\n",
      "        [7., 7., 7., 7., 8., 8., 8.]])\n",
      "torch.Size([16, 49])\n",
      "torch.Size([16, 49, 49])\n",
      "attn mask shape torch.Size([16, 49, 49])\n",
      "==============================\n",
      "swinT block\n",
      "x shape torch.Size([2, 784, 192])\n",
      "x_windows shape torch.Size([32, 7, 7, 192])\n",
      "x_windows shape torch.Size([32, 49, 192])\n",
      "x_window qkv shape torch.Size([32, 49, 576])\n",
      "x_window qkv shape torch.Size([32, 49, 3, 6, 32])\n",
      "x_window qkv shape torch.Size([3, 32, 6, 49, 32])\n",
      "q k v shape torch.Size([32, 6, 49, 32])\n",
      "attn shape torch.Size([32, 6, 49, 49])\n",
      "attn window shape torch.Size([32, 6, 49, 32])\n",
      "attn window shape torch.Size([32, 49, 192])\n",
      "attn window shape torch.Size([32, 7, 7, 192])\n",
      "shift x shape torch.Size([2, 4, 4, 7, 7, 192])\n",
      "shift x shape torch.Size([2, 4, 7, 4, 7, 192])\n",
      "shift x shape torch.Size([2, 28, 28, 192])\n",
      "x shape torch.Size([2, 784, 192])\n",
      "==============================\n",
      "shift swinT block\n",
      "x shape torch.Size([2, 784, 192])\n",
      "x view shape torch.Size([2, 28, 28, 192])\n",
      "shift x shape torch.Size([2, 28, 28, 192])\n",
      "x window shape torch.Size([2, 4, 7, 4, 7, 192])\n",
      "x window shape torch.Size([32, 7, 7, 192])\n",
      "x window shape torch.Size([32, 49, 192])\n",
      "x_window qkv shape torch.Size([32, 49, 576])\n",
      "x_window qkv shape torch.Size([32, 49, 3, 6, 32])\n",
      "x_window qkv shape torch.Size([3, 32, 6, 49, 32])\n",
      "q k v shape torch.Size([32, 6, 49, 32])\n",
      "attn shape torch.Size([32, 6, 49, 49])\n",
      "attn mask shape torch.Size([16, 49, 49])\n",
      "attn view shape torch.Size([2, 16, 6, 49, 49])\n",
      "attn mask unsqueeze shape torch.Size([1, 16, 1, 49, 49])\n",
      "attn in mask shape torch.Size([2, 16, 6, 49, 49])\n",
      "attn view shape torch.Size([32, 6, 49, 49])\n",
      "attn window shape torch.Size([32, 6, 49, 32])\n",
      "attn windows shape torch.Size([32, 49, 192])\n",
      "attn windows shape torch.Size([32, 7, 7, 192])\n",
      "shift x shape torch.Size([2, 4, 4, 7, 7, 192])\n",
      "shift x shape torch.Size([2, 4, 7, 4, 7, 192])\n",
      "shift x shape torch.Size([2, 28, 28, 192])\n",
      "roll back x shape torch.Size([2, 28, 28, 192])\n",
      "x view shape torch.Size([2, 784, 192])\n",
      "==============================\n",
      "downsample\n",
      "x shape torch.Size([2, 784, 192])\n",
      "x view shape torch.Size([2, 28, 28, 192])\n",
      "x cat shape torch.Size([2, 14, 14, 768])\n",
      "x view shape torch.Size([2, 196, 768])\n",
      "x reduction shape torch.Size([2, 196, 384])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1,3,224,224))\n",
    "print(x.shape)\n",
    "\n",
    "# patch embed\n",
    "print('patch embed')\n",
    "patch_size=4\n",
    "in_c=3\n",
    "embed_dim=96\n",
    "proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "_, _, H, W = x.shape\n",
    "print(x.shape)\n",
    "x = proj(x)\n",
    "_, _, H, W = x.shape\n",
    "print(x.shape)\n",
    "x = x.flatten(2).transpose(1,2)\n",
    "print(x.shape)\n",
    "print('H,W:',H,W)\n",
    "\n",
    "print('='*50)\n",
    "print('basic layer [stage2 swinT block and stage3 patch merging]')  # 96, 192\n",
    "H,W = 28,28\n",
    "print('H,W:',H,W)\n",
    "x = torch.rand(2,28*28,192)\n",
    "print(x.shape)\n",
    "print('='*30)\n",
    "print('mask')\n",
    "img_mask = torch.zeros((1,H,W,1))\n",
    "h_slices = (slice(0,-7),slice(-7,-3),slice(-3,None))\n",
    "w_slices = (slice(0,-7),slice(-7,-3),slice(-3,None))\n",
    "cnt = 0\n",
    "for h in h_slices:\n",
    "    for w in w_slices:\n",
    "        img_mask[:,h,w,:] = cnt\n",
    "        cnt += 1\n",
    "print(img_mask[0,-7:-3,-7:-3,0])\n",
    "print(img_mask.shape)\n",
    "\n",
    "windows = img_mask.view(1,H//7,7,W//7,7,1)\n",
    "print(windows.shape)\n",
    "\n",
    "mask_windows = windows.permute(0,1,3,2,4,5).contiguous().view(-1,7,7,1)\n",
    "print(mask_windows.shape)\n",
    "print(mask_windows[-1,:,:,0])\n",
    "\n",
    "mask_windows = mask_windows.view(-1,7*7)\n",
    "print(mask_windows.shape)\n",
    "\n",
    "attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "print(attn_mask.shape)\n",
    "# print(attn_mask[-1,:])\n",
    "\n",
    "attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "print('attn mask shape',attn_mask.shape)\n",
    "print('='*30)\n",
    "print('swinT block')\n",
    "print('x shape',x.shape)\n",
    "shortcut = x \n",
    "x = x.view(2,H,W,192)\n",
    "attn_mask_none = None\n",
    "shifted_x = x\n",
    "window_size = 7\n",
    "shifted_x = shifted_x.view(2,H//7,7,W//7,7,192)\n",
    "x_windows = shifted_x.permute(0,1,3,2,4,5).contiguous().view(-1,7,7,192)\n",
    "print('x_windows shape',x_windows.shape)\n",
    "x_windows = x_windows.view(-1,7*7,192)\n",
    "print('x_windows shape',x_windows.shape)\n",
    "B_,N,C = x_windows.shape\n",
    "qkv = nn.Linear(192, 192 * 3, bias=False)\n",
    "x_window_qkv = qkv(x_windows)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "num_heads  = 6\n",
    "x_window_qkv = x_window_qkv.reshape(B_,N,3,num_heads,192//num_heads)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "x_window_qkv = x_window_qkv.permute(2,0,3,1,4)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "q,k,v = x_window_qkv.unbind(0)\n",
    "print('q k v shape',q.shape)\n",
    "attn = q @ k.transpose(-2,-1)\n",
    "print('attn shape',attn.shape)\n",
    "attn_window = attn @ v\n",
    "print('attn window shape',attn_window.shape)\n",
    "attn_window = attn_window.transpose(1,2).reshape(B_,N,C)\n",
    "print('attn window shape',attn_window.shape)\n",
    "attn_window = attn_window.view(-1,7,7,192)\n",
    "print('attn window shape',attn_window.shape)\n",
    "B = int(attn_window.shape[0] / (H*W/7/7))\n",
    "shift_x =  attn_window.view(B,H//7,W//7,7,7,-1)\n",
    "print('shift x shape',shift_x.shape)\n",
    "shift_x = shift_x.permute(0,1,3,2,4,5).contiguous()\n",
    "print('shift x shape',shift_x.shape)\n",
    "shift_x = shift_x.view(B,H,W,-1)\n",
    "print('shift x shape',shift_x.shape)\n",
    "x = shift_x.view(B,H*W,C)\n",
    "print('x shape',x.shape)\n",
    "\n",
    "print('='*30)\n",
    "print('shift swinT block')\n",
    "print('x shape',x.shape)\n",
    "x = x.view(B,H,W,C) \n",
    "print('x view shape',x.shape)\n",
    "shift_x = torch.roll(x,shifts=(-3,-3),dims=(1,2))\n",
    "print('shift x shape',shift_x.shape)\n",
    "B,H,W,C = shift_x.shape\n",
    "x_windows = shift_x.view(B,H//7,7,W//7,7,C)\n",
    "print('x window shape',x_windows.shape)\n",
    "x_windows = x_windows.permute(0,1,3,2,4,5).contiguous().view(-1,7,7,C)\n",
    "print('x window shape',x_windows.shape)\n",
    "x_windows = x_windows.view(-1,7*7,C)\n",
    "print('x window shape',x_windows.shape)\n",
    "B_, N, C = x_windows.shape\n",
    "qkv = nn.Linear(192, 192 * 3, bias=False)\n",
    "x_window_qkv = qkv(x_windows)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "num_heads  = 6\n",
    "x_window_qkv = x_window_qkv.reshape(B_,N,3,num_heads,192//num_heads)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "x_window_qkv = x_window_qkv.permute(2,0,3,1,4)\n",
    "print('x_window qkv shape',x_window_qkv.shape)\n",
    "q,k,v = x_window_qkv.unbind(0)\n",
    "print('q k v shape',q.shape)\n",
    "attn = q @ k.transpose(-2, -1)\n",
    "print('attn shape',attn.shape)\n",
    "print('attn mask shape',attn_mask.shape)\n",
    "nw = attn_mask.shape[0]\n",
    "attn = attn.view(B_//nw,nw,num_heads,N,N)\n",
    "print('attn view shape',attn.shape)\n",
    "attn_mask = attn_mask.unsqueeze(1).unsqueeze(0)\n",
    "print('attn mask unsqueeze shape',attn_mask.shape)\n",
    "attn = attn + attn_mask\n",
    "print('attn in mask shape',attn.shape)\n",
    "attn = attn.view(-1,num_heads,N,N)\n",
    "print('attn view shape',attn.shape)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "attn = softmax(attn)\n",
    "attn_windows = (attn @ v)\n",
    "print('attn window shape',attn_windows.shape)\n",
    "attn_windows = attn_windows.transpose(1,2).reshape(B_,N,C)\n",
    "print('attn windows shape',attn_windows.shape)\n",
    "attn_windows = attn_windows.view(-1,7,7,C)\n",
    "print('attn windows shape',attn_windows.shape)\n",
    "B = int(attn_windows.shape[0]/H/W*7*7)\n",
    "shift_x = attn_windows.view(B,H//7,W//7,7,7,C)\n",
    "print('shift x shape',shift_x.shape)\n",
    "shift_x = shift_x.permute(0,1,3,2,4,5)\n",
    "print('shift x shape',shift_x.shape)\n",
    "shift_x = shift_x.contiguous().view(B,H,W,-1)\n",
    "print('shift x shape',shift_x.shape)\n",
    "x = torch.roll(shift_x,shifts=(3,3),dims=(1,2))\n",
    "print('roll back x shape',x.shape)\n",
    "x = x.view(B,H*W,C)\n",
    "print('x view shape',x.shape)\n",
    "print('='*30)\n",
    "print('downsample')\n",
    "B,L,C = x.shape\n",
    "print('x shape',x.shape)\n",
    "x = x.view(B,H,W,C)\n",
    "print('x view shape',x.shape)\n",
    "x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n",
    "x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n",
    "x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n",
    "x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n",
    "x = torch.cat([x0,x1,x2,x3],-1)\n",
    "print('x cat shape',x.shape)\n",
    "x = x.view(B,-1,4*C)\n",
    "print('x view shape',x.shape)\n",
    "reduction = nn.Linear(4 * 192, 2 * 192, bias=False)\n",
    "x = reduction(x)\n",
    "print('x reduction shape',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24, 25, 26, 27],\n",
      "        [28, 29, 30, 31, 32, 33, 34, 35, 36],\n",
      "        [37, 38, 39, 40, 41, 42, 43, 44, 45],\n",
      "        [46, 47, 48, 49, 50, 51, 52, 53, 54],\n",
      "        [55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
      "        [64, 65, 66, 67, 68, 69, 70, 71, 72],\n",
      "        [73, 74, 75, 76, 77, 78, 79, 80, 81]])\n",
      "tensor([[11, 12, 13, 14, 15, 16, 17, 18, 10],\n",
      "        [20, 21, 22, 23, 24, 25, 26, 27, 19],\n",
      "        [29, 30, 31, 32, 33, 34, 35, 36, 28],\n",
      "        [38, 39, 40, 41, 42, 43, 44, 45, 37],\n",
      "        [47, 48, 49, 50, 51, 52, 53, 54, 46],\n",
      "        [56, 57, 58, 59, 60, 61, 62, 63, 55],\n",
      "        [65, 66, 67, 68, 69, 70, 71, 72, 64],\n",
      "        [74, 75, 76, 77, 78, 79, 80, 81, 73],\n",
      "        [ 2,  3,  4,  5,  6,  7,  8,  9,  1]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,82).reshape(9,9)\n",
    "print(x)\n",
    "shift_x = torch.roll(x,shifts=(-1,-1),dims=(0,1))\n",
    "print(shift_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d0b924d46175125b2288e35b7e96bc220e2e674c3c0a3172562eac2975018cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
